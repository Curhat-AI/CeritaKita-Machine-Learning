{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8566906,"sourceType":"datasetVersion","datasetId":5121737},{"sourceId":8568170,"sourceType":"datasetVersion","datasetId":5122768}],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom transformers import BertTokenizer, TFBertForSequenceClassification\n\nfile_path = '/kaggle/input/emotions/emotions.csv'\ndataset = pd.read_csv(file_path)\n\ndataset.dropna(inplace=True)\n\ndef clean_text(text):\n    text = re.sub(r'\\[USERNAME\\]', '', text)\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    text = text.lower()\n    text = text.strip()\n    return text\n\ndataset['cleaned_text'] = dataset['text'].apply(clean_text)\n\nlabel_encoder = LabelEncoder()\ndataset['label'] = label_encoder.fit_transform(dataset['label'])\n\nprint(\"Sample counts per class before under-sampling:\")\nprint(dataset['label'].value_counts())\n\ndef under_sampling(df, label_col):\n    min_count = df[label_col].value_counts().min()\n    sampled_df = df.groupby(label_col).apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n    return sampled_df\n\ndataset = under_sampling(dataset, 'label')\n\nprint(\"\\nSample counts per class after under-sampling:\")\nprint(dataset['label'].value_counts())\n\nX_train, X_test, y_train, y_test = train_test_split(dataset['cleaned_text'], dataset['label'], test_size=0.3, random_state=42, stratify=dataset['label'])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-31T08:34:51.979196Z","iopub.execute_input":"2024-05-31T08:34:51.979555Z","iopub.status.idle":"2024-05-31T08:35:13.243663Z","shell.execute_reply.started":"2024-05-31T08:34:51.979514Z","shell.execute_reply":"2024-05-31T08:35:13.242717Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-31 08:34:55.500905: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-31 08:34:55.501047: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-31 08:34:55.620426: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Sample counts per class before under-sampling:\nlabel\n1    141067\n0    121187\n3     57317\n4     47712\n2     34554\n5     14972\nName: count, dtype: int64\n\nSample counts per class after under-sampling:\nlabel\n0    14972\n1    14972\n2    14972\n3    14972\n4    14972\n5    14972\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/1599797030.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  sampled_df = df.groupby(label_col).apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = \"bert-base-multilingual-cased\" \ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n\ndef encode_texts(texts, tokenizer, max_len=128):\n    return tokenizer(\n        texts.tolist(),\n        max_length=max_len,\n        truncation=True,\n        padding='max_length',\n        return_tensors='tf'\n    )\n\ntrain_encodings = encode_texts(X_train, tokenizer)\ntest_encodings = encode_texts(X_test, tokenizer)\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(train_encodings),\n    y_train.values\n)).shuffle(len(X_train)).batch(16)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(test_encodings),\n    y_test.values\n)).batch(16)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T08:35:13.245215Z","iopub.execute_input":"2024-05-31T08:35:13.245522Z","iopub.status.idle":"2024-05-31T08:36:15.754217Z","shell.execute_reply.started":"2024-05-31T08:35:13.245483Z","shell.execute_reply":"2024-05-31T08:36:15.753437Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddaf57c85d3a4e78b8b85e3eb3d60971"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"072eb78bad49414988ccfb3228b0238c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"148cc08c9e004257be87bdc0ed39f596"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c9a7a78f6de4534b8394bd177d69a8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28e7f62fbc654c288b171d06c1d4dde2"}},"metadata":{}},{"name":"stderr","text":"All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n\nSome weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nmodel.compile(optimizer=optimizer, \n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n              metrics=['accuracy'])\n\nhistory = model.fit(train_dataset,\n                    epochs=1,\n                    validation_data=test_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T08:36:15.755344Z","iopub.execute_input":"2024-05-31T08:36:15.755634Z","iopub.status.idle":"2024-05-31T09:10:37.763012Z","shell.execute_reply.started":"2024-05-31T08:36:15.755608Z","shell.execute_reply":"2024-05-31T09:10:37.762073Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"WARNING: AutoGraph could not transform <function infer_framework at 0x7cb789aa3010> and will run it as-is.\nCause: for/else statement not yet supported\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1717144656.539351     107 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"3931/3931 [==============================] - 2062s 500ms/step - loss: 0.3162 - accuracy: 0.8844 - val_loss: 0.1307 - val_accuracy: 0.9485\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install googletrans==4.0.0-rc1","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:10:37.765612Z","iopub.execute_input":"2024-05-31T09:10:37.765889Z","iopub.status.idle":"2024-05-31T09:10:55.741233Z","shell.execute_reply.started":"2024-05-31T09:10:37.765865Z","shell.execute_reply":"2024-05-31T09:10:55.739944Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting googletrans==4.0.0-rc1\n  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.2.2)\nCollecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading hstspreload-2024.5.1-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\nCollecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\nCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\nCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\nCollecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\nCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\nCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\nCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\nCollecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\nDownloading httpx-0.13.3-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\nDownloading hstspreload-2024.5.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\nDownloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\nBuilding wheels for collected packages: googletrans\n  Building wheel for googletrans (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=8562512899b3ec6433afd566ad04626ad41eda0544af48691bfcd8263d1a5503\n  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\nSuccessfully built googletrans\n\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n  Attempting uninstall: h11\n    Found existing installation: h11 0.14.0\n    Uninstalling h11-0.14.0:\n      Successfully uninstalled h11-0.14.0\n  Attempting uninstall: idna\n    Found existing installation: idna 3.6\n    Uninstalling idna-3.6:\n      Successfully uninstalled idna-3.6\n  Attempting uninstall: httpcore\n    Found existing installation: httpcore 1.0.5\n    Uninstalling httpcore-1.0.5:\n      Successfully uninstalled httpcore-1.0.5\n  Attempting uninstall: httpx\n    Found existing installation: httpx 0.27.0\n    Uninstalling httpx-0.27.0:\n      Successfully uninstalled httpx-0.27.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyter-server-proxy 4.1.0 requires aiohttp, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\njupyterlab 4.2.1 requires httpx>=0.25.0, but you have httpx 0.13.3 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.5.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from googletrans import Translator\n\nemotion_labels = {\n    0: 'sadness',\n    1: 'joy',\n    2: 'love',\n    3: 'anger',\n    4: 'fear',\n    5: 'surprise'\n}\n\ndef translate_texts(texts, src_lang='id', dest_lang='en'):\n    translator = Translator()\n    translated_texts = []\n    for text in texts:\n        translation = translator.translate(text, src=src_lang, dest=dest_lang)\n        translated_texts.append(translation.text)\n        print(f'Translated Text: {translation.text}')  # Menampilkan hasil terjemahan\n    return translated_texts\n\ndef predict_new_texts(model, tokenizer, texts, label_encoder, max_len=128):\n    translated_texts = translate_texts(texts)\n    clean_texts = [clean_text(text) for text in translated_texts]\n\n    inputs = tokenizer(clean_texts, max_length=max_len, truncation=True, padding='max_length', return_tensors='tf')\n\n    pred_prob = model.predict(inputs)\n    pred_classes = np.argmax(pred_prob.logits, axis=1)\n    \n    labels = [emotion_labels[pred] for pred in pred_classes]\n    return labels\n\nnew_texts = [\n    \"Saya merasa sangat sedih dan kesepian akhir-akhir ini, tidak tahu harus berbuat apa.\",\n    \"Saya sangat marah dan kecewa dengan teman-teman saya.\",\n    \"Hari ini saya merasa sangat bahagia dan penuh semangat!\",\n    \"Saya merasa takut dan cemas setiap kali memikirkan masa depan.\",\n    \"Saya merasa sangat bangga dengan pencapaian saya hari ini.\",\n    \"Saya merasa hampa dan tidak termotivasi untuk melakukan apa pun.\",\n    \"Saya merasa begitu dicintai dan diperhatikan oleh keluarga saya.\",\n    \"Saya merasa kesal dengan situasi di tempat kerja saya.\",\n    \"Saya merasa penuh harapan dan optimis tentang peluang baru ini.\",\n    \"Saya merasa bersalah dan menyesal tentang kesalahan yang saya buat.\"\n]\n\npredicted_labels = predict_new_texts(model, tokenizer, new_texts, label_encoder)\nfor text, label in zip(new_texts, predicted_labels):\n    print(f'Text: {text}\\nPredicted Label: {label}\\n')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:10:55.742774Z","iopub.execute_input":"2024-05-31T09:10:55.743066Z","iopub.status.idle":"2024-05-31T09:11:13.037048Z","shell.execute_reply.started":"2024-05-31T09:10:55.743039Z","shell.execute_reply":"2024-05-31T09:11:13.036161Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Translated Text: I feel very sad and lonely lately, don't know what to do.\nTranslated Text: I am very angry and disappointed with my friends.\nTranslated Text: Today I feel very happy and full of enthusiasm!\nTranslated Text: I feel scared and anxious every time I think of the future.\nTranslated Text: I feel very proud of my achievements today.\nTranslated Text: I feel empty and not motivated to do anything.\nTranslated Text: I feel so loved and cared for by my family.\nTranslated Text: I was annoyed with the situation at my workplace.\nTranslated Text: I feel full and optimistic about this new opportunity.\nTranslated Text: I feel guilty and sorry about the mistakes I made.\n1/1 [==============================] - 10s 10s/step\nText: Saya merasa sangat sedih dan kesepian akhir-akhir ini, tidak tahu harus berbuat apa.\nPredicted Label: sadness\n\nText: Saya sangat marah dan kecewa dengan teman-teman saya.\nPredicted Label: anger\n\nText: Hari ini saya merasa sangat bahagia dan penuh semangat!\nPredicted Label: joy\n\nText: Saya merasa takut dan cemas setiap kali memikirkan masa depan.\nPredicted Label: fear\n\nText: Saya merasa sangat bangga dengan pencapaian saya hari ini.\nPredicted Label: joy\n\nText: Saya merasa hampa dan tidak termotivasi untuk melakukan apa pun.\nPredicted Label: sadness\n\nText: Saya merasa begitu dicintai dan diperhatikan oleh keluarga saya.\nPredicted Label: love\n\nText: Saya merasa kesal dengan situasi di tempat kerja saya.\nPredicted Label: anger\n\nText: Saya merasa penuh harapan dan optimis tentang peluang baru ini.\nPredicted Label: joy\n\nText: Saya merasa bersalah dan menyesal tentang kesalahan yang saya buat.\nPredicted Label: sadness\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n\ny_pred = model.predict(test_dataset)\ny_pred_labels = np.argmax(y_pred.logits, axis=1)\n\ncm = confusion_matrix(y_test, y_pred_labels)\n\ntarget_names = dataset['label'].unique().astype(str).tolist()\n\nevaluation_report = classification_report(y_test, y_pred_labels, target_names=target_names)\n\nf1 = f1_score(y_test, y_pred_labels, average='weighted')\n\nprecision = precision_score(y_test, y_pred_labels, average='weighted')\nrecall = recall_score(y_test, y_pred_labels, average='weighted')\n\naccuracy = accuracy_score(y_test, y_pred_labels)\n\nprint(\"Confusion Matrix:\")\nprint(cm)\n\nprint(\"\\nEvaluation Report:\")\nprint(evaluation_report)\n\nprint(\"\\nF1 Score:\", f1)\nprint(\"Precision:\", precision)\nprint(\"Sensitivity (Recall):\", recall)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:30:15.356667Z","iopub.execute_input":"2024-05-31T09:30:15.357046Z","iopub.status.idle":"2024-05-31T09:34:24.332908Z","shell.execute_reply.started":"2024-05-31T09:30:15.357016Z","shell.execute_reply":"2024-05-31T09:34:24.331855Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"1685/1685 [==============================] - 248s 147ms/step\nConfusion Matrix:\n[[4269    8    1  132   78    3]\n [  11 4036  357   26   20   41]\n [   3    2 4483    2    1    1]\n [  18   10    8 4264  192    0]\n [  23    5    2   40 4032  390]\n [   2    5    0    1    6 4478]]\n\nEvaluation Report:\n              precision    recall  f1-score   support\n\n           0       0.99      0.95      0.97      4491\n           1       0.99      0.90      0.94      4491\n           2       0.92      1.00      0.96      4492\n           3       0.95      0.95      0.95      4492\n           4       0.93      0.90      0.91      4492\n           5       0.91      1.00      0.95      4492\n\n    accuracy                           0.95     26950\n   macro avg       0.95      0.95      0.95     26950\nweighted avg       0.95      0.95      0.95     26950\n\n\nF1 Score: 0.9483115710294252\nPrecision: 0.9502338158910858\nSensitivity (Recall): 0.9484972170686456\nAccuracy: 0.9484972170686456\n","output_type":"stream"}]}]}