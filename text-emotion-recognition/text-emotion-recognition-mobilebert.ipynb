{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8598069,"sourceType":"datasetVersion","datasetId":5143892}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom transformers import MobileBertTokenizer, TFMobileBertForSequenceClassification\n\nfile_path = '/kaggle/input/emotions1/emotions.csv'\ndataset = pd.read_csv(file_path)\n\ndataset.dropna(inplace=True)\n\ndef clean_text(text):\n    text = re.sub(r'\\[USERNAME\\]', '', text)\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    text = text.lower()\n    text = text.strip()\n    return text\n\ndataset['cleaned_text'] = dataset['text'].apply(clean_text)\n\nlabel_encoder = LabelEncoder()\ndataset['label'] = label_encoder.fit_transform(dataset['label'])\n\nprint(\"Sample counts per class before under-sampling:\")\nprint(dataset['label'].value_counts())\n\ndef under_sampling(df, label_col):\n    min_count = df[label_col].value_counts().min()\n    sampled_df = df.groupby(label_col).apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n    return sampled_df\n\ndataset = under_sampling(dataset, 'label')\n\nprint(\"\\nSample counts per class after under-sampling:\")\nprint(dataset['label'].value_counts())\n\nX_train, X_test, y_train, y_test = train_test_split(dataset['cleaned_text'], dataset['label'], test_size=0.3, random_state=42, stratify=dataset['label'])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-03T22:06:13.543923Z","iopub.execute_input":"2024-06-03T22:06:13.544341Z","iopub.status.idle":"2024-06-03T22:06:37.989072Z","shell.execute_reply.started":"2024-06-03T22:06:13.544314Z","shell.execute_reply":"2024-06-03T22:06:37.988010Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-03 22:06:18.195133: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-03 22:06:18.195228: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-03 22:06:18.357335: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Sample counts per class before under-sampling:\nlabel\n1    141067\n0    121187\n3     57317\n4     47712\n2     34554\n5     14972\nName: count, dtype: int64\n\nSample counts per class after under-sampling:\nlabel\n0    14972\n1    14972\n2    14972\n3    14972\n4    14972\n5    14972\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/723102189.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  sampled_df = df.groupby(label_col).apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = \"google/mobilebert-uncased\"\ntokenizer = MobileBertTokenizer.from_pretrained(model_name)\nmodel = TFMobileBertForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n\ndef encode_texts(texts, tokenizer, max_len=128):\n    return tokenizer(\n        texts.tolist(),\n        max_length=max_len,\n        truncation=True,\n        padding='max_length',\n        return_tensors='tf'\n    )\n\ntrain_encodings = encode_texts(X_train, tokenizer)\ntest_encodings = encode_texts(X_test, tokenizer)\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(train_encodings),\n    y_train.values\n)).shuffle(len(X_train)).batch(16)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(test_encodings),\n    y_test.values\n)).batch(16)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T22:07:06.644446Z","iopub.execute_input":"2024-06-03T22:07:06.644830Z","iopub.status.idle":"2024-06-03T22:08:26.891721Z","shell.execute_reply.started":"2024-06-03T22:07:06.644802Z","shell.execute_reply":"2024-06-03T22:08:26.890852Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6defdb6397834c15b001bd46e74285e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebe87d4e010f45c0b382a85c8d6cc352"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/847 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0de53fc3bf334c25a89be4526575eea7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tf_model.h5:   0%|          | 0.00/164M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d0516f5c0a640629d5ef540fb5d96ff"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFMobileBertForSequenceClassification.\n\nSome layers of TFMobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nmodel.compile(optimizer=optimizer, \n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n              metrics=['accuracy'])\n\nhistory = model.fit(train_dataset,\n                    epochs=7,\n                    validation_data=test_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T22:08:47.523258Z","iopub.execute_input":"2024-06-03T22:08:47.523880Z","iopub.status.idle":"2024-06-04T00:49:34.092737Z","shell.execute_reply.started":"2024-06-03T22:08:47.523849Z","shell.execute_reply":"2024-06-04T00:49:34.091832Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Epoch 1/7\nWARNING: AutoGraph could not transform <function infer_framework at 0x793161b0b0a0> and will run it as-is.\nCause: for/else statement not yet supported\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1717452867.743334     110 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"3931/3931 [==============================] - 1813s 356ms/step - loss: 8720.6875 - accuracy: 0.1871 - val_loss: 1.8273 - val_accuracy: 0.2277\nEpoch 2/7\n3931/3931 [==============================] - 1339s 341ms/step - loss: 2.4056 - accuracy: 0.2538 - val_loss: 1.5931 - val_accuracy: 0.3314\nEpoch 3/7\n3931/3931 [==============================] - 1313s 334ms/step - loss: 1.5194 - accuracy: 0.3698 - val_loss: 1.2534 - val_accuracy: 0.5101\nEpoch 4/7\n3931/3931 [==============================] - 1295s 329ms/step - loss: 0.8822 - accuracy: 0.6677 - val_loss: 0.4620 - val_accuracy: 0.8477\nEpoch 5/7\n3931/3931 [==============================] - 1293s 329ms/step - loss: 0.3483 - accuracy: 0.8805 - val_loss: 0.2156 - val_accuracy: 0.9233\nEpoch 6/7\n3931/3931 [==============================] - 1299s 331ms/step - loss: 0.1909 - accuracy: 0.9275 - val_loss: 0.1571 - val_accuracy: 0.9446\nEpoch 7/7\n3931/3931 [==============================] - 1294s 329ms/step - loss: 0.3936 - accuracy: 0.9418 - val_loss: 0.1297 - val_accuracy: 0.9479\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install googletrans==4.0.0-rc1","metadata":{"execution":{"iopub.status.busy":"2024-06-04T00:50:40.287485Z","iopub.execute_input":"2024-06-04T00:50:40.288153Z","iopub.status.idle":"2024-06-04T00:50:58.027280Z","shell.execute_reply.started":"2024-06-04T00:50:40.288119Z","shell.execute_reply":"2024-06-04T00:50:58.026006Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting googletrans==4.0.0-rc1\n  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.2.2)\nCollecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading hstspreload-2024.6.1-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\nCollecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\nCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\nCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\nCollecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\nCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\nCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\nCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\nCollecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\nDownloading httpx-0.13.3-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\nDownloading hstspreload-2024.6.1-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\nDownloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\nBuilding wheels for collected packages: googletrans\n  Building wheel for googletrans (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=c3adfb2bb4d188b78f1476271cec89af8b3041b61cc02808d5c2a4ef62c6fbf2\n  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\nSuccessfully built googletrans\nInstalling collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n  Attempting uninstall: h11\n    Found existing installation: h11 0.14.0\n    Uninstalling h11-0.14.0:\n      Successfully uninstalled h11-0.14.0\n  Attempting uninstall: idna\n    Found existing installation: idna 3.6\n    Uninstalling idna-3.6:\n      Successfully uninstalled idna-3.6\n  Attempting uninstall: httpcore\n    Found existing installation: httpcore 1.0.5\n    Uninstalling httpcore-1.0.5:\n      Successfully uninstalled httpcore-1.0.5\n  Attempting uninstall: httpx\n    Found existing installation: httpx 0.27.0\n    Uninstalling httpx-0.27.0:\n      Successfully uninstalled httpx-0.27.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\njupyterlab 4.1.6 requires httpx>=0.25.0, but you have httpx 0.13.3 which is incompatible.\njupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.6.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from googletrans import Translator\n\nemotion_labels = {\n    0: 'sadness',\n    1: 'joy',\n    2: 'love',\n    3: 'anger',\n    4: 'fear',\n    5: 'surprise'\n}\n\ndef translate_texts(texts, src_lang='id', dest_lang='en'):\n    translator = Translator()\n    translated_texts = []\n    for text in texts:\n        translation = translator.translate(text, src=src_lang, dest=dest_lang)\n        translated_texts.append(translation.text)\n        print(f'Translated Text: {translation.text}')  # Menampilkan hasil terjemahan\n    return translated_texts\n\ndef predict_new_texts(model, tokenizer, texts, label_encoder, max_len=128):\n    translated_texts = translate_texts(texts)\n    clean_texts = [clean_text(text) for text in translated_texts]\n\n    inputs = tokenizer(clean_texts, max_length=max_len, truncation=True, padding='max_length', return_tensors='tf')\n\n    pred_prob = model.predict(inputs)\n    pred_classes = np.argmax(pred_prob.logits, axis=1)\n    \n    labels = [emotion_labels[pred] for pred in pred_classes]\n    return labels\n\nnew_texts = [\n    \"Saya merasa sangat sedih dan kesepian akhir-akhir ini, tidak tahu harus berbuat apa.\",\n    \"Saya sangat marah dan kecewa dengan teman-teman saya.\",\n    \"Hari ini saya merasa sangat bahagia dan penuh semangat!\",\n    \"Saya merasa takut dan cemas setiap kali memikirkan masa depan.\",\n    \"Saya merasa sangat bangga dengan pencapaian saya hari ini.\",\n    \"Saya merasa hampa dan tidak termotivasi untuk melakukan apa pun.\",\n    \"Saya merasa begitu dicintai dan diperhatikan oleh keluarga saya.\",\n    \"Saya merasa kesal dengan situasi di tempat kerja saya.\",\n    \"Saya merasa takut terhadap peluang baru ini.\",\n    \"Saya merasa bersalah dan menyesal tentang kesalahan yang saya buat.\",\n    \"Saya merasa sangat mencintai pasangan saya begitu juga sebaliknya\"\n]\n\npredicted_labels = predict_new_texts(model, tokenizer, new_texts, label_encoder)\nfor text, label in zip(new_texts, predicted_labels):\n    print(f'Text: {text}\\nPredicted Label: {label}\\n')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T00:51:58.942562Z","iopub.execute_input":"2024-06-04T00:51:58.943463Z","iopub.status.idle":"2024-06-04T00:52:45.633421Z","shell.execute_reply.started":"2024-06-04T00:51:58.943426Z","shell.execute_reply":"2024-06-04T00:52:45.632356Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Translated Text: I feel very sad and lonely lately, don't know what to do.\nTranslated Text: I am very angry and disappointed with my friends.\nTranslated Text: Today I feel very happy and full of enthusiasm!\nTranslated Text: I feel scared and anxious every time I think of the future.\nTranslated Text: I feel very proud of my achievements today.\nTranslated Text: I feel empty and not motivated to do anything.\nTranslated Text: I feel so loved and cared for by my family.\nTranslated Text: I was annoyed with the situation at my workplace.\nTranslated Text: I feel afraid of this new opportunity.\nTranslated Text: I feel guilty and sorry about the mistakes I made.\nTranslated Text: I feel very loving my partner and vice versa\n1/1 [==============================] - 40s 40s/step\nText: Saya merasa sangat sedih dan kesepian akhir-akhir ini, tidak tahu harus berbuat apa.\nPredicted Label: sadness\n\nText: Saya sangat marah dan kecewa dengan teman-teman saya.\nPredicted Label: anger\n\nText: Hari ini saya merasa sangat bahagia dan penuh semangat!\nPredicted Label: joy\n\nText: Saya merasa takut dan cemas setiap kali memikirkan masa depan.\nPredicted Label: fear\n\nText: Saya merasa sangat bangga dengan pencapaian saya hari ini.\nPredicted Label: joy\n\nText: Saya merasa hampa dan tidak termotivasi untuk melakukan apa pun.\nPredicted Label: sadness\n\nText: Saya merasa begitu dicintai dan diperhatikan oleh keluarga saya.\nPredicted Label: love\n\nText: Saya merasa kesal dengan situasi di tempat kerja saya.\nPredicted Label: anger\n\nText: Saya merasa takut terhadap peluang baru ini.\nPredicted Label: fear\n\nText: Saya merasa bersalah dan menyesal tentang kesalahan yang saya buat.\nPredicted Label: sadness\n\nText: Saya merasa sangat mencintai pasangan saya begitu juga sebaliknya\nPredicted Label: love\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n\ny_pred = model.predict(test_dataset)\ny_pred_labels = np.argmax(y_pred.logits, axis=1)\n\ncm = confusion_matrix(y_test, y_pred_labels)\n\ntarget_names = dataset['label'].unique().astype(str).tolist()\n\nevaluation_report = classification_report(y_test, y_pred_labels, target_names=target_names)\n\nf1 = f1_score(y_test, y_pred_labels, average='weighted')\n\nprecision = precision_score(y_test, y_pred_labels, average='weighted')\nrecall = recall_score(y_test, y_pred_labels, average='weighted')\n\naccuracy = accuracy_score(y_test, y_pred_labels)\n\nprint(\"Confusion Matrix:\")\nprint(cm)\n\nprint(\"\\nEvaluation Report:\")\nprint(evaluation_report)\n\nprint(\"\\nF1 Score:\", f1)\nprint(\"Precision:\", precision)\nprint(\"Sensitivity (Recall):\", recall)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T00:53:34.128339Z","iopub.execute_input":"2024-06-04T00:53:34.128738Z","iopub.status.idle":"2024-06-04T00:58:11.336107Z","shell.execute_reply.started":"2024-06-04T00:53:34.128707Z","shell.execute_reply":"2024-06-04T00:58:11.335158Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"1685/1685 [==============================] - 276s 141ms/step\nConfusion Matrix:\n[[4306   11    3   98   69    4]\n [  18 4060  332   23   15   43]\n [   7   14 4465    6    0    0]\n [  71   13    3 4349   56    0]\n [  31    2    1  199 3884  375]\n [   3    5    2    0    1 4481]]\n\nEvaluation Report:\n              precision    recall  f1-score   support\n\n           0       0.97      0.96      0.96      4491\n           1       0.99      0.90      0.94      4491\n           2       0.93      0.99      0.96      4492\n           3       0.93      0.97      0.95      4492\n           4       0.96      0.86      0.91      4492\n           5       0.91      1.00      0.95      4492\n\n    accuracy                           0.95     26950\n   macro avg       0.95      0.95      0.95     26950\nweighted avg       0.95      0.95      0.95     26950\n\n\nF1 Score: 0.9474276140186353\nPrecision: 0.9496553703799208\nSensitivity (Recall): 0.9478664192949907\nAccuracy: 0.9478664192949907\n","output_type":"stream"}]},{"cell_type":"code","source":"converter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_quantized_model = converter.convert()\n\ntflite_model_path = 'model_text_mobile.tflite'\nwith open(tflite_model_path, 'wb') as f:\n    f.write(tflite_quantized_model)\n    \nfrom IPython.display import FileLink\nFileLink(tflite_model_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T01:11:26.427476Z","iopub.execute_input":"2024-06-04T01:11:26.428401Z","iopub.status.idle":"2024-06-04T01:18:56.531696Z","shell.execute_reply.started":"2024-06-04T01:11:26.428366Z","shell.execute_reply":"2024-06-04T01:18:56.530671Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Summary on the non-converted ops:\n---------------------------------\n * Accepted dialects: tfl, builtin, func\n * Non-Converted Ops: 773, Total Ops 5434, % non-converted = 14.23 %\n * 773 ARITH ops\n\n- arith.constant:  773 occurrences  (f32: 752, i32: 21)\n\n\n\n  (f32: 748)\n  (f32: 48)\n  (f32: 1)\n  (f32: 1, i32: 289)\n  (f32: 3)\n  (i32: 1)\n  (f32: 362)\n  (uq_8: 3, i32: 578)\n  (f32: 218)\n\n  (i32: 292)\n  (f32: 2)\n  (uq_8: 365)\n  (i32: 1)\n  (i32: 578)\n  (f32: 746, i32: 1)\n  (i32: 292)\n  (f32: 24)\n  (f32: 3, i32: 4)\n  (f32: 1)\n  (f32: 96)\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/model_text_mobile.tflite","text/html":"<a href='model_text_mobile.tflite' target='_blank'>model_text_mobile.tflite</a><br>"},"metadata":{}}]}]}